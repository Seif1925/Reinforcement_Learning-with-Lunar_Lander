
# ğŸš€ Deep Q-Network (DQN) for LunarLander-v3

This project implements a **Deep Q-Network (DQN)** to solve the **LunarLander-v3** environment from OpenAI Gym.  
The agent learns to land the lunar module safely using reinforcement learning.

---


## ğŸ¥ The enhancement of the agent progress

ğŸ‘‰ [Watch Demo](media/video.gif)

---


## ğŸ“Œ Concept

The **main idea** behind DQN is to combine:
1. **Q-Learning** â†’ estimating the value of actions given states.  
2. **Deep Neural Networks** â†’ to approximate the Q-function.  
3. **Experience Replay** â†’ storing past experiences in a buffer to sample from them randomly, which breaks correlation and improves learning.  
4. **Target Network** â†’ a fixed copy of the Q-network that is updated slowly (soft update) for more stable training.

---

## ğŸ—ï¸ Network Architecture

The **DQN** uses a simple **feedforward neural network** with 2 hidden layers:

- **Input Layer** â†’ takes the state (8 values in LunarLander).  
- **Hidden Layer 1** â†’ 64 neurons, ReLU activation.  
- **Hidden Layer 2** â†’ 64 neurons, ReLU activation.  
- **Output Layer** â†’ number of possible actions (4 in LunarLander).  

```python
class DQNNetwork(nn.Module):
    def __init__(self, state_size, action_size, seed=42):
        super(DQNNetwork, self).__init__()
        self.seed = torch.manual_seed(seed)
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_size)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)
```

---

## âš™ï¸ Hyperparameters

| Hyperparameter        | Value        | Description |
|-----------------------|-------------|-------------|
| Replay Buffer Size    | `100,000`   | Maximum number of past experiences stored |
| Batch Size            | `100`       | Number of samples per training step |
| Discount Factor (Î³)   | `0.99`      | How much future rewards are valued |
| Learning Rate (Î±)     | `5e-4`      | Step size for optimizer |
| Update Frequency      | `4` steps   | How often the network is updated |
| Soft Update (Ï„)       | `1e-3`      | Rate of slowly updating target network |

---

## ğŸ”‘ Important Functions

### 1. Soft Update (stabilizing training)
The **soft update** ensures that the target network is updated slowly towards the local network, which avoids large fluctuations.

```python
def soft_update(local_model, target_model, tau):
    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):
        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)
```

â¡ï¸ **Explanation**:  
- `tau` is a small number (e.g., `1e-3`).  
- Instead of replacing the weights completely, we blend them:  
  `Î¸_target â† Ï„*Î¸_local + (1-Ï„)*Î¸_target`  

---

### 2. Learning Step (update rule)
The **learning step** samples from the replay buffer and updates the Q-values.

```python
def learn(experiences, gamma):
    states, actions, rewards, next_states, dones = experiences

    Q_targets_next = target_model(next_states).detach().max(1)[0].unsqueeze(1)
    Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))

    Q_expected = local_model(states).gather(1, actions)

    loss = F.mse_loss(Q_expected, Q_targets)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

â¡ï¸ **Explanation**:  
- `Q_expected` = predicted Q-values.  
- `Q_targets` = actual Q-values using target network.  
- Minimize **MSE loss** between them.

---

## ğŸ“Š Training Results

- **Learning Curve**: Below is the **loss and reward per episode**.  
- The agent learns to land successfully after 400 episode.

![Training Curve](media/LossAndReward.jpg)

---


## ğŸ“Œ How to Run

```bash
pip install -r requirements.txt
python main.py
```

---

## âœ… Summary

- DQN learns by combining **neural networks, replay buffer, and target network**.  
- **Soft update** stabilizes training.  
- The agent improves gradually as shown in the reward curve and demo video.

---
